{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bbc825-573d-48f9-8b82-886feffb37e1",
   "metadata": {},
   "outputs": [],
   "source": [
    " %cd /sci/labs/yotamd/lab_share/avishai.wizel/eRNA/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ef59de-c8f3-4de6-af28-04704f0e303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "from scipy.sparse import issparse\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow import keras\n",
    "import tensorflow_addons as tfa\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e11c41-88f2-4253-a46d-1f7af59febab",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6385dfd5-a670-42fe-abd0-b12e2723541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_rna = ad.read_h5ad('./10X_PBMC/03_filtered_data/filtered_rna_adata.h5ad')\n",
    "sc_atac = ad.read_h5ad(\"./10X_PBMC/03_filtered_data/filtered_atac_adata.h5ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2f00df-d7a2-486f-932b-e45fbe0fc8b9",
   "metadata": {},
   "source": [
    "# Main parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a76d8f-f1b6-40fd-9970-8cc9dafd2649",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_rna_var_genes = 15000\n",
    "top_var_peaks = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dc0035-c148-4fe2-994b-c051d1b89e01",
   "metadata": {},
   "source": [
    "# Filters cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659e135c-7707-48b9-b4a3-41ebbd2729a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_cells_by_qc_metrics(\n",
    "    adata: ad.AnnData,\n",
    "    min_genes: int = 200,\n",
    "    max_genes: int = 2500,\n",
    "    min_counts: int = 1000,\n",
    "    max_mt_pct: float = 40.0\n",
    ") -> ad.AnnData:\n",
    "    \"\"\"\n",
    "    Filters cells based on standard quality control metrics:\n",
    "    number of genes detected, total counts, and mitochondrial gene percentage.\n",
    "\n",
    "    Args:\n",
    "        adata (anndata.AnnData):\n",
    "            The AnnData object containing raw gene expression counts (cells x genes).\n",
    "            Assumes mitochondrial genes are prefixed with 'MT-' (human) or 'mt-' (mouse).\n",
    "        min_genes (int): Minimum number of genes expressed per cell.\n",
    "        max_genes (int): Maximum number of genes expressed per cell (to remove doublets).\n",
    "        min_counts (int): Minimum total counts per cell.\n",
    "        max_mt_pct (float): Maximum allowed percentage of mitochondrial counts per cell.\n",
    "\n",
    "    Returns:\n",
    "        anndata.AnnData: A new AnnData object with filtered cells.\n",
    "                         QC metrics are added to adata.obs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure adata.var['mt'] is set if not already\n",
    "    if 'mt' not in adata.var:\n",
    "        # Assuming human data with 'MT-' prefix. Adjust for mouse ('mt-') or other.\n",
    "        adata.var['mt'] = adata.var_names.str.startswith('MT-') \n",
    "\n",
    "    print(\"Calculating QC metrics...\")\n",
    "    sc.pp.calculate_qc_metrics(\n",
    "        adata, \n",
    "        qc_vars=['mt'], \n",
    "        percent_top=None, \n",
    "        log1p=False, \n",
    "        inplace=True\n",
    "    )\n",
    "\n",
    "    print(f\"Original number of cells: {adata.n_obs}\")\n",
    "    \n",
    "    # Apply filters\n",
    "    initial_cells_count = adata.n_obs\n",
    "    \n",
    "    # Combine filtering criteria using boolean logic\n",
    "    cells_to_keep = (\n",
    "        (adata.obs['n_genes_by_counts'] >= min_genes) &\n",
    "        (adata.obs['n_genes_by_counts'] <= max_genes) &\n",
    "        (adata.obs['total_counts'] >= min_counts) &\n",
    "        (adata.obs['pct_counts_mt'] <= max_mt_pct)\n",
    "    )\n",
    "    \n",
    "    adata_filtered = adata[cells_to_keep, :].copy()\n",
    "    \n",
    "    filtered_cells_count = adata_filtered.n_obs\n",
    "    removed_cells_count = initial_cells_count - filtered_cells_count\n",
    "    \n",
    "    print(f\"Cells removed: {removed_cells_count}\")\n",
    "    print(f\"Number of cells after filtering: {filtered_cells_count}\")\n",
    "\n",
    "    return adata_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea4e912-ed37-458f-a116-8251b5932c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_filtered_qc = filter_cells_by_qc_metrics(\n",
    "    sc_rna.copy(),\n",
    "    min_genes=200,\n",
    "    max_genes=2500,\n",
    "    min_counts=500,\n",
    "    max_mt_pct=25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2e4d4d-ae6d-4735-a5f9-233c7a4cdf15",
   "metadata": {},
   "source": [
    "# Normalize scRNA-seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd6a6c9-bd56-46ee-93e2-79c6b69e47c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Size Normalization\n",
    "sc.pp.normalize_total(adata_filtered_qc, target_sum=1e4)\n",
    "\n",
    "# 2. Log-transformation (log1p)\n",
    "sc.pp.log1p(adata_filtered_qc)\n",
    "sc.pp.highly_variable_genes(adata_filtered_qc, n_top_genes=top_rna_var_genes, flavor='seurat')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e744883-e093-4c91-b26c-66191eac0528",
   "metadata": {},
   "source": [
    "Take only high variable genes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8804d63c-0d6a-4a88-b008-d7d911cecc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_rna_filtered = adata_filtered_qc[:, adata_filtered_qc.var.highly_variable].copy()\n",
    "\n",
    "# filter atac cells based on filtered rna cells\n",
    "filtered_cell_barcodes = sc_rna_filtered.obs_names\n",
    "adata_atac_filtered = sc_atac[sc_atac.obs_names.isin(filtered_cell_barcodes), :].copy()\n",
    "adata_atac_filtered = sc_atac[filtered_cell_barcodes, :].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415743d4-c767-4a72-a1ce-b50844ce8220",
   "metadata": {},
   "source": [
    "# Filter for highly variable peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1a611e-b47a-4aeb-999b-ec4641cd61d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_highly_variable_peaks(adata_atac: ad.AnnData, n_top_peaks: int = 10000) -> ad.AnnData:\n",
    "    \"\"\"\n",
    "    Identifies and keeps the top N highly variable peaks in an AnnData object\n",
    "    based on their variance-to-mean ratio (a proxy for dispersion).\n",
    "\n",
    "    Args:\n",
    "        adata_atac (anndata.AnnData):\n",
    "            The AnnData object containing ATAC-seq data (cells x peaks).\n",
    "            Assumes adata_atac.X contains counts or binarized values.\n",
    "        n_top_peaks (int):\n",
    "            The number of top highly variable peaks to select.\n",
    "\n",
    "    Returns:\n",
    "        anndata.AnnData: An AnnData object subsetted to include only the selected top N peaks.\n",
    "                         Statistical information is added to adata_atac.var.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Original AnnData shape (cells x peaks): {adata_atac.shape}\")\n",
    "\n",
    "    # Convert to dense for calculation if sparse, using float32 for memory efficiency\n",
    "    data = adata_atac.X.toarray().astype(np.float32) if issparse(adata_atac.X) else adata_atac.X.astype(np.float32)\n",
    "    \n",
    "    # Calculate mean and variance for each peak (column-wise)\n",
    "    peak_means = np.mean(data, axis=0)\n",
    "    peak_variances = np.var(data, axis=0)\n",
    "    \n",
    "    # Avoid division by zero for peaks with zero mean\n",
    "    # A common approach is to add a small constant to the mean or handle NaN results.\n",
    "    # For ATAC (binary), peaks with zero mean have zero variance and are not variable.\n",
    "    # We'll set dispersion to 0 for these or filter them out.\n",
    "    \n",
    "    # Calculate variance-to-mean ratio as a measure of dispersion\n",
    "    # Adding a small epsilon to avoid division by zero\n",
    "    epsilon = 1e-6 \n",
    "    dispersion = peak_variances / (peak_means + epsilon)\n",
    "    \n",
    "    # Store these metrics in adata.var\n",
    "    adata_atac.var['peak_means'] = peak_means\n",
    "    adata_atac.var['peak_variances'] = peak_variances\n",
    "    adata_atac.var['peak_dispersion'] = dispersion\n",
    "\n",
    "    # Rank peaks by dispersion and select top N\n",
    "    # Sort in descending order\n",
    "    adata_atac.var['ranked_dispersion'] = adata_atac.var['peak_dispersion'].rank(ascending=False, method='first')\n",
    "    \n",
    "    selected_peaks = adata_atac.var[adata_atac.var['ranked_dispersion'] <= n_top_peaks].index\n",
    "    \n",
    "    adata_filtered_peaks = adata_atac[:, selected_peaks].copy()\n",
    "    \n",
    "    print(f\"AnnData shape after filtering to top {n_top_peaks} highly variable peaks: {adata_filtered_peaks.shape}\")\n",
    "    \n",
    "    return adata_filtered_peaks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ac36b1-2bc7-4b43-b189-b4917fbc3b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_hvps = get_top_n_highly_variable_peaks(adata_atac_filtered.copy(), top_var_peaks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391fc8d8-722a-4d8f-8f6b-7888df048d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "scRNA = sc_rna_filtered.X\n",
    "scATAC_binary = adata_hvps.X.toarray()\n",
    "del(sc_rna)\n",
    "del(sc_atac)\n",
    "del(adata_atac_filtered)\n",
    "del(sc_rna_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8592cbf7-25e1-4c37-9a7a-c0aa35986b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"scRNA dim (cells X genes):\" ,scRNA.shape)\n",
    "print(\"scATAC dim (cells X peaks):\" ,scATAC_binary.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95dc1a2-ccd8-4491-b193-e49b1a867f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the RNA-seq data\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(scRNA.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe82b7c3-1699-4f6e-b89d-21ed37df211c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_y = MinMaxScaler()\n",
    "y_scaled = scaler_y.fit_transform(scATAC_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117eb14d-15ad-4ccd-bcb8-fa79bb8c8bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbecd281-0a37-4bdf-bcdb-0371ca950d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the total number of zeros and ones across all peaks combined\n",
    "# # This flattens the array and then counts occurrences of 0s and 1s.\n",
    "# num_zeros = np.sum(y_train == 0)\n",
    "# num_ones = np.sum(y_train == 1)\n",
    "# total_elements = num_zeros + num_ones # Total number of 0s and 1s combined in y_train\n",
    "\n",
    "# # Calculate class weights\n",
    "# # These weights are inversely proportional to the class frequencies.\n",
    "# # This assigns a higher weight to the less frequent class (typically '1's in imbalanced data)\n",
    "# # to make the model pay more attention to correctly classifying them.\n",
    "# # The 'total_elements / 2.0' part ensures the sum of weights for a balanced dataset\n",
    "# # would ideally be around 1, helping to stabilize the loss scale.\n",
    "\n",
    "# # Ensure no division by zero if a class is completely absent (though rare for 0/1)\n",
    "# weight_for_0 = (total_elements / (10.0 * num_zeros)) if num_zeros > 0 else 1.0\n",
    "# weight_for_1 = (total_elements / (2.0 * num_ones)) if num_ones > 0 else 1.0\n",
    "\n",
    "# # Store the calculated weights in a dictionary format required by Keras\n",
    "# class_weights = {0: weight_for_0, 1: weight_for_1}\n",
    "# print(f\"Computed class weights: {class_weights}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d0072d-9dd2-45a0-a45a-162aeb4bf591",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 2. Building the Keras Neural Network Model ---\n",
    "\n",
    "# Define model parameters\n",
    "input_dim = X_train.shape[1]  # Number of genes (input features)\n",
    "output_dim = y_train.shape[1] # Number of ATAC genomic locations (output targets)\n",
    "\n",
    "# Build the Sequential Keras model\n",
    "model = keras.Sequential([\n",
    "    # Input Layer: Defines the shape of the input data\n",
    "    layers.Input(shape=(input_dim,)),\n",
    "\n",
    "    # Hidden Layers (Dense - Fully Connected)\n",
    "    # Common to start with larger layers and gradually decrease units.\n",
    "    # 'relu' (Rectified Linear Unit) is a common activation function for hidden layers.\n",
    "    layers.Dense(units=512, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.2), # Dropout layer for regularization (prevents overfitting), dropping 20% of neurons\n",
    "    layers.Dense(units=256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(units=128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    # You can add more layers here if needed\n",
    "\n",
    "    # Output Layer\n",
    "    # The number of units must match the number of ATAC locations (output_dim).\n",
    "    # Activation function:\n",
    "    # - 'linear' (or no activation) for continuous regression output (e.g., after StandardScaler).\n",
    "    # - 'sigmoid' if your 'y' targets are scaled between 0 and 1 (e.g., after MinMaxScaler)\n",
    "    #   or if they are binary (0/1) representing probabilities.\n",
    "     layers.Dense(units=output_dim, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "# Optimizer: 'adam' is a popular and efficient optimizer.\n",
    "# Loss function:\n",
    "# - 'mse' (Mean Squared Error) is standard for regression problems.\n",
    "# - 'mae' (Mean Absolute Error) is another common choice for regression.\n",
    "# - If your output is binary (0/1) with 'sigmoid' activation, use 'binary_crossentropy' for loss.\n",
    "optimizer=keras.optimizers.SGD(learning_rate=0.01, momentum=0.9) # Start with 0.01 for LR\n",
    "\n",
    "# model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['mae', 'accuracy']) # Using MAE as an additional metric\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    \n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=1)\n",
    "    union = tf.reduce_sum(y_true, axis=1) + tf.reduce_sum(y_pred, axis=1)\n",
    "    \n",
    "    dice_coef = (2. * intersection + smooth) / (union + smooth)\n",
    "    loss = 1 - dice_coef\n",
    "    \n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "    \n",
    "def combined_loss(y_true, y_pred):\n",
    "    focal = tfa.losses.SigmoidFocalCrossEntropy(gamma=2.0, alpha=0.75)(y_true, y_pred)\n",
    "    d_loss = dice_loss(y_true, y_pred)\n",
    "    return focal + d_loss\n",
    "    \n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),  # Can lower to 1e-4 if data is noisy\n",
    "    loss=combined_loss,\n",
    "    metrics=[\n",
    "        tf.keras.metrics.AUC(\n",
    "            curve='PR',         # Precision-Recall AUC\n",
    "            multi_label=True,   # Required for multi-label output\n",
    "            num_labels=output_dim,  # Set to your output vector size\n",
    "            name='pr_auc'\n",
    "        ),\n",
    "        tf.keras.metrics.BinaryAccuracy(name='bin_acc'),   # Overall binary accuracy\n",
    "        tf.keras.metrics.Precision(name='precision'),      # How many predicted positives were correct\n",
    "        tf.keras.metrics.Recall(name='recall')             # How many actual positives were found\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print a summary of the model architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c905ab-f320-40cd-8cd3-b831b99d2cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 3. Training the Model ---\n",
    "\n",
    "print(\"\\nStarting model training...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,          # Number of times to iterate over the entire training dataset\n",
    "    batch_size=32,      # Number of samples per gradient update\n",
    "    validation_split=0.1, # Fraction of the training data to be used as validation data\n",
    "                          # This helps monitor performance on unseen data during training.\n",
    "    verbose=1,           # Display progress bar during training\n",
    "    # class_weight=class_weights\n",
    ")\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b772e-541f-4f35-b2fc-a751bce92432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_from_history(history):\n",
    "    metrics = ['precision', 'recall', 'pr_auc']\n",
    "    for metric in metrics:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(history.history[metric], label=f\"Train {metric}\")\n",
    "        plt.plot(history.history[f\"val_{metric}\"], label=f\"Val {metric}\")\n",
    "        plt.title(f\"{metric.upper()} Over Epochs\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(metric.upper())\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "plot_metrics_from_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a89aef-5a3a-46c8-8a50-6db0a6125046",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probs = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d3080d-e3a0-403a-b633-87fa471dadf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "average_precisions = []\n",
    "\n",
    "for i in range(y_test.shape[1]):\n",
    "    precision, recall, _ = precision_recall_curve(y_test[:, i], y_pred_probs[:, i])\n",
    "    ap = average_precision_score(y_test[:, i], y_pred_probs[:, i])\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    average_precisions.append(ap)\n",
    "\n",
    "# ממוצע מדדי AP לכל הפיקים\n",
    "mean_ap = np.mean(average_precisions)\n",
    "print(f\"Mean Average Precision (mAP): {mean_ap:.4f}\")\n",
    "\n",
    "# לצורך פלוט ממוצע – אופציה פשוטה היא להשתמש ב-micro-average PR curve:\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Flatten arrays עבור micro-average\n",
    "y_true_flat = y_test.ravel()\n",
    "y_scores_flat = y_pred_probs.ravel()\n",
    "\n",
    "precision_micro, recall_micro, _ = precision_recall_curve(y_true_flat, y_scores_flat)\n",
    "average_precision_micro = average_precision_score(y_true_flat, y_scores_flat)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall_micro, precision_micro, label=f'micro-average PR curve (AP = {average_precision_micro:.4f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall curve (Micro-average)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8761cc9e-24e9-4531-8e2e-4afaed198194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
